{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734095ab-f26f-4adf-8b2f-39235ef73d50",
   "metadata": {},
   "source": [
    "# ğŸ“‚ Repository Discovery & Preliminary EDA\n",
    "\n",
    "This notebook performs an initial scan of the data repository to:\n",
    "\n",
    "- Identify all files and subdirectories\n",
    "- Surface CSV, Excel, and text files\n",
    "- Prepare for exploratory data analysis (EDA)\n",
    "\n",
    "**Directory to explore: (replace with your directory)**\n",
    "\n",
    "`/Users/jasminemotupalli/Library/CloudStorage/GoogleDrive-jasmine@dataloveco.com/My Drive/Personal/0 - U of Denver/4 - Term Summer 25/FIN-6305 Quant Methods III/Cody`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809556f",
   "metadata": {},
   "source": [
    "### ğŸ”§ Step 1: Set Your Working Directory\n",
    "\n",
    "This is the folder on your computer where all the data files live.  \n",
    "ğŸ“Œ **Edit the path below** to match your local folder.  \n",
    "\n",
    "> âœ… Tip: You can drag and drop the folder into JupyterLab to see the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e829c3b1-6d08-43e4-919d-15dcda44f2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/jasminemotupalli/Library/CloudStorage/GoogleDrive-jasmine@dataloveco.com/My Drive/Personal/0 - U of Denver/4 - Term Summer 25/FIN-6305 Quant Methods III/Cody')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the base directory path (change to whatever path your notebook is located)\n",
    "base_path = Path(\n",
    "    \"/Users/jasminemotupalli/Library/CloudStorage/GoogleDrive-jasmine@dataloveco.com/\"\n",
    "    \"My Drive/Personal/0 - U of Denver/4 - Term Summer 25/FIN-6305 Quant Methods III/Cody\"\n",
    ")\n",
    "\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c77e0",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 2: List All Files in the Folder\n",
    "\n",
    "This block scans your folder and prints out all files and subfolders.  \n",
    "No need to change anything here â€” just run it to verify your files are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb55266-8019-4358-9117-a72fb0ccb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Total items found: 90\n",
      "ğŸ“„ Total files: 82\n",
      "ğŸ“ Total folders: 8\n",
      "\n",
      "ğŸ“Š File extension breakdown (top 10):\n",
      "      .csv: 39 files\n",
      "     .xlsx: 27 files\n",
      "    .ipynb: 7 files\n",
      "      .pdf: 2 files\n",
      "      .zip: 1 files\n",
      "      .txt: 1 files\n",
      "       .db: 1 files\n",
      "     .yaml: 1 files\n",
      "\n",
      "ğŸ—‚ï¸ Sample files:\n",
      "  - Full_ROI_Dataset.csv\n",
      "  - College_Scorecard_Raw_Data_05192025.zip\n",
      "  - .DS_Store\n",
      "  - data_dictionary.xlsx\n",
      "  - Cody Data Discovery - Annotated.ipynb\n",
      "  - multi_tab_variable_names.xlsx\n",
      "  - Cody Data Diâ€¦ - JupyterLab.pdf\n",
      "  - Cody Data Discovery.ipynb\n",
      "  - discovered_data_files.txt\n",
      "  - descriptive_summaries.xlsx\n",
      "\n",
      "ğŸ“‚ Sample folders:\n",
      "  - anaconda_projects\n",
      "  - .virtual_documents\n",
      "  - College_Scorecard_Raw_Data_05192025\n",
      "  - .ipynb_checkpoints\n",
      "  - anaconda_projects/db\n"
     ]
    }
   ],
   "source": [
    "# Recursively list all items (files + folders)\n",
    "all_items = list(base_path.rglob(\"*\"))\n",
    "print(f\"ğŸ” Total items found: {len(all_items)}\")\n",
    "\n",
    "# Separate into files and directories\n",
    "all_files = [p for p in all_items if p.is_file()]\n",
    "all_dirs = [p for p in all_items if p.is_dir()]\n",
    "\n",
    "print(f\"ğŸ“„ Total files: {len(all_files)}\")\n",
    "print(f\"ğŸ“ Total folders: {len(all_dirs)}\")\n",
    "\n",
    "# Count file extensions\n",
    "file_extensions = Counter(p.suffix.lower() for p in all_files if p.suffix)\n",
    "print(\"\\nğŸ“Š File extension breakdown (top 10):\")\n",
    "for ext, count in file_extensions.most_common(10):\n",
    "    print(f\"  {ext or '[no extension]':>8}: {count} files\")\n",
    "\n",
    "# Preview a few files\n",
    "print(\"\\nğŸ—‚ï¸ Sample files:\")\n",
    "for f in all_files[:10]:\n",
    "    print(f\"  - {f.relative_to(base_path)}\")\n",
    "\n",
    "# Preview a few folders\n",
    "print(\"\\nğŸ“‚ Sample folders:\")\n",
    "for d in all_dirs[:5]:\n",
    "    print(f\"  - {d.relative_to(base_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35e88b-bc10-4f2f-9473-f3b5422e6409",
   "metadata": {},
   "source": [
    "### ğŸ“„ Step 3: Filter to Spreadsheet and Text Data Files\n",
    "\n",
    "This block filters your folder for files that are likely to contain data â€” specifically:\n",
    "\n",
    "- `.csv` (Comma-Separated Values)\n",
    "- `.txt` (Plain Text)\n",
    "- `.xlsx` or `.xls` (Excel files)\n",
    "\n",
    "These are the most common formats used in data science and analytics.\n",
    "\n",
    "ğŸ“Œ **Want to include other types?**  \n",
    "You can add additional file types by editing this list in the code block:  \n",
    "```python\n",
    "data_extensions = [\".csv\", \".txt\", \".xlsx\", \".xls\"]\n",
    "```\n",
    "> ğŸ§ª Example: To include `.json` files (JavaScript Object Notation), change the line to:\n",
    "```python\n",
    "data_extensions = [\".csv\", \".txt\", \".xlsx\", \".xls\", \".json\"]\n",
    "```\n",
    "\n",
    "Only files with these extensions will be included in the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349f398f-c734-4fad-aea4-5df944366245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Total explorable data files found: 67\n",
      "\n",
      "ğŸ“ Breakdown by file type:\n",
      "  - .csv   : 39 files\n",
      "  - .txt   : 1 files\n",
      "  - .xlsx  : 27 files\n",
      "  - .xls   : 0 files\n",
      "\n",
      "ğŸ“‚ Sample data files:\n",
      "  - Full_ROI_Dataset.csv\n",
      "  - data_dictionary.xlsx\n",
      "  - multi_tab_variable_names.xlsx\n",
      "  - discovered_data_files.txt\n",
      "  - descriptive_summaries.xlsx\n",
      "  - College_Scorecard_Raw_Data_05192025/MERGED2011_12_PP.csv\n",
      "  - College_Scorecard_Raw_Data_05192025/FieldOfStudyData1819_1920_PP.csv\n",
      "  - College_Scorecard_Raw_Data_05192025/MERGED1999_00_PP.csv\n",
      "  - College_Scorecard_Raw_Data_05192025/MERGED2006_07_PP.csv\n",
      "  - College_Scorecard_Raw_Data_05192025/MERGED2014_15_PP.csv\n",
      "\n",
      "âš ï¸ Note: 15 files were skipped (not recognized as data files).\n",
      "    Example skipped file: College_Scorecard_Raw_Data_05192025.zip\n"
     ]
    }
   ],
   "source": [
    "# Define supported data extensions\n",
    "data_extensions = [\".csv\", \".txt\", \".xlsx\", \".xls\"]\n",
    "\n",
    "# Filter files based on known data types\n",
    "data_files = [f for f in all_files if f.suffix.lower() in data_extensions]\n",
    "print(f\"ğŸ“Š Total explorable data files found: {len(data_files)}\")\n",
    "\n",
    "# Group data files by file type\n",
    "ext_counts = defaultdict(int)\n",
    "for f in data_files:\n",
    "    ext_counts[f.suffix.lower()] += 1\n",
    "\n",
    "# Show counts by type\n",
    "print(\"\\nğŸ“ Breakdown by file type:\")\n",
    "for ext in data_extensions:\n",
    "    print(f\"  - {ext:6} : {ext_counts[ext]} files\")\n",
    "\n",
    "# Preview the first few data files\n",
    "print(\"\\nğŸ“‚ Sample data files:\")\n",
    "for f in data_files[:10]:\n",
    "    print(f\"  - {f.relative_to(base_path)}\")\n",
    "\n",
    "# Warn about other files that were skipped\n",
    "non_data_files = [f for f in all_files if f.is_file() and f.suffix.lower() not in data_extensions]\n",
    "if non_data_files:\n",
    "    print(f\"\\nâš ï¸ Note: {len(non_data_files)} files were skipped (not recognized as data files).\")\n",
    "    print(\"    Example skipped file:\", non_data_files[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402edeb",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Step 4: Export the File List\n",
    "\n",
    "This saves a text file listing all the data files found.  \n",
    "You can open this later to double-check which files are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37a6e74-430a-47cf-b172-61704402e183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ File list exported to: /Users/jasminemotupalli/Library/CloudStorage/GoogleDrive-jasmine@dataloveco.com/My Drive/Personal/0 - U of Denver/4 - Term Summer 25/FIN-6305 Quant Methods III/Cody/discovered_data_files.txt\n"
     ]
    }
   ],
   "source": [
    "# Write the list of data files to a text file\n",
    "output_path = base_path / \"discovered_data_files.txt\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"Explorable Data Files Found:\\n\\n\")\n",
    "    for file in data_files:\n",
    "        f.write(str(file) + \"\\n\")\n",
    "\n",
    "print(f\"âœ“ File list exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887e083-6145-4d0a-a160-43d7afaf02df",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 5: Export Column Names from All Files to One Excel Workbook\n",
    "\n",
    "This section loops through every discovered file and:\n",
    "\n",
    "1. Loads just the **first row** of each dataset to extract the column names.\n",
    "2. Creates a new Excel workbook (`multi_tab_variable_names.xlsx`) with:\n",
    "   - One **tab per file**\n",
    "   - Each tab listing the **column names** in that dataset\n",
    "\n",
    "ğŸ§  **Why this is useful:**  \n",
    "It gives you a compact snapshot of the schema/structure across all your datasets, without needing to open or load them fully.\n",
    "\n",
    "ğŸ“Œ **Note:**\n",
    "- Duplicate sheet names (case-insensitive) are automatically handled by appending suffixes.\n",
    "- Any files that error out will still generate a tab with the error message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1654d-b272-436e-8407-a29ea593b280",
   "metadata": {},
   "source": [
    "### ğŸ§¾ Step 6: Generate an Excel Workbook with Column Names for Each Dataset\n",
    "\n",
    "This step creates a single Excel file called `multi_tab_variable_names.xlsx` where:\n",
    "\n",
    "- Each **sheet represents one dataset**\n",
    "- Each sheet lists the **column names** (variables) from that dataset\n",
    "\n",
    "ğŸ” **What this helps you do:**\n",
    "\n",
    "- Quickly preview the structure of each file without opening them individually\n",
    "- Identify overlapping columns, track schema consistency, and prepare for further analysis\n",
    "\n",
    "ğŸ› ï¸ **How it works:**\n",
    "\n",
    "- Loads just the first row of each dataset to extract column headers\n",
    "- Ensures unique sheet names (automatically appends suffixes if needed)\n",
    "- If a file canâ€™t be read, a sheet with an error message is added\n",
    "\n",
    "ğŸ“Œ You do **not** need to change anything in the code unless:\n",
    "- You want to use a different output filename\n",
    "- You need to support other file types (like `.json`)\n",
    "\n",
    "The resulting Excel file provides a compact, readable summary of all dataset schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1435bae9-fd9c-48e9-8ef6-25b65dcf7fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ New variable name Excel file created: multi_tab_variable_names.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load discovered file paths\n",
    "file_path = \"discovered_data_files.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    all_lines = f.readlines()\n",
    "\n",
    "# Get valid file paths\n",
    "data_file_paths = [\n",
    "    line.strip() for line in all_lines\n",
    "    if line.strip().endswith((\".csv\", \".xlsx\", \".xls\"))\n",
    "]\n",
    "\n",
    "# Create a new Excel writer\n",
    "output_excel = \"multi_tab_variable_names.xlsx\"\n",
    "writer = pd.ExcelWriter(output_excel, engine=\"xlsxwriter\")\n",
    "\n",
    "# Track lowercase sheet names to avoid duplicates (Excel is case-insensitive)\n",
    "used_sheet_names = set()\n",
    "\n",
    "for path in data_file_paths:\n",
    "    try:\n",
    "        # Load first row to get column names\n",
    "        if path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(path, nrows=1)\n",
    "        elif path.endswith((\".xlsx\", \".xls\")):\n",
    "            df = pd.read_excel(path, nrows=1)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Prepare the dataframe of column names\n",
    "        columns_df = pd.DataFrame({\"column_name\": df.columns})\n",
    "\n",
    "        # Generate base sheet name\n",
    "        base_name = os.path.basename(path).split(\".\")[0][:28]  # room for suffix\n",
    "        sheet_name = base_name\n",
    "        suffix = 1\n",
    "\n",
    "        # Ensure uniqueness (case-insensitive)\n",
    "        while sheet_name.lower() in used_sheet_names:\n",
    "            sheet_name = f\"{base_name}_{suffix}\"[:31]\n",
    "            suffix += 1\n",
    "\n",
    "        used_sheet_names.add(sheet_name.lower())\n",
    "\n",
    "        # Write to Excel\n",
    "        columns_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_df = pd.DataFrame({\"column_name\": [f\"ERROR: {str(e)}\"]})\n",
    "        sheet_name = f\"error_{os.path.basename(path)[:25]}\"\n",
    "        writer.sheets[sheet_name] = writer.book.add_worksheet(sheet_name)\n",
    "        error_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# Save the workbook\n",
    "writer.close()\n",
    "\n",
    "print(f\"âœ“ New variable name Excel file created: {output_excel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cbd5e-1610-45ba-9311-d196a5ad4a98",
   "metadata": {},
   "source": [
    "### ğŸ§¾ Step 7: Export One Sheet per File with Column Names\n",
    "\n",
    "This step creates a second Excel file, `multi_tab_variable_names.xlsx`, where:\n",
    "\n",
    "- Each sheet corresponds to one dataset\n",
    "- Only the **column names** (variable headers) are listed â€” no sample values or data types\n",
    "- Sheet names are auto-adjusted to avoid naming conflicts\n",
    "\n",
    "ğŸ” This file serves as a quick reference for:\n",
    "- Reviewing dataset structure at a glance\n",
    "- Comparing variables across files\n",
    "- Supporting documentation or planning for data cleaning\n",
    "\n",
    "ğŸ§  This step **complements** the earlier `data_dictionary.xlsx`, which contains richer metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb175125-268d-4340-9da4-7db38e86d41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Processing file: Full_ROI_Dataset.csv\n",
      "âœ… Loaded: Full_ROI_Dataset.csv with 30023 rows and 68 columns\n",
      "ğŸ“Š Summary written to sheet: Full_ROI_Dataset.csv\n",
      "\n",
      "ğŸ“‚ Processing file: data_dictionary.xlsx\n",
      "âœ… Loaded: data_dictionary.xlsx with 100733 rows and 4 columns\n",
      "âŒ Error processing data_dictionary.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: multi_tab_variable_names.xlsx\n",
      "âœ… Loaded: multi_tab_variable_names.xlsx with 68 rows and 1 columns\n",
      "âŒ Error processing multi_tab_variable_names.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: descriptive_summaries.xlsx\n",
      "âŒ Error processing descriptive_summaries.xlsx: Excel file format cannot be determined, you must specify an engine manually.\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2011_12_PP.csv\n",
      "âœ… Loaded: MERGED2011_12_PP.csv with 7746 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2011_12_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: FieldOfStudyData1819_1920_PP.csv\n",
      "âœ… Loaded: FieldOfStudyData1819_1920_PP.csv with 233948 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: FieldOfStudyData1819_1920_PP\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED1999_00_PP.csv\n",
      "âœ… Loaded: MERGED1999_00_PP.csv with 6609 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED1999_00_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2006_07_PP.csv\n",
      "âœ… Loaded: MERGED2006_07_PP.csv with 6951 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2006_07_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2014_15_PP.csv\n",
      "âœ… Loaded: MERGED2014_15_PP.csv with 7766 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2014_15_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: Most-Recent-Cohorts-Institution.csv\n",
      "âœ… Loaded: Most-Recent-Cohorts-Institution.csv with 6429 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: Most-Recent-Cohorts-Institut\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2003_04_PP.csv\n",
      "âœ… Loaded: MERGED2003_04_PP.csv with 6673 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2003_04_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED1996_97_PP.csv\n",
      "âœ… Loaded: MERGED1996_97_PP.csv with 7007 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED1996_97_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: FieldOfStudyData1516_1617_PP.csv\n",
      "âœ… Loaded: FieldOfStudyData1516_1617_PP.csv with 219202 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: FieldOfStudyData1516_1617_PP\n",
      "\n",
      "ğŸ“‚ Processing file: Most-Recent-Cohorts-Field-of-Study.csv\n",
      "âœ… Loaded: Most-Recent-Cohorts-Field-of-Study.csv with 229188 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: Most-Recent-Cohorts-Field-of\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2012_13_PP.csv\n",
      "âœ… Loaded: MERGED2012_13_PP.csv with 7862 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2012_13_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2020_21_PP.csv\n",
      "âœ… Loaded: MERGED2020_21_PP.csv with 6681 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2020_21_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2005_06_PP.csv\n",
      "âœ… Loaded: MERGED2005_06_PP.csv with 6899 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2005_06_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2009_10_PP.csv\n",
      "âœ… Loaded: MERGED2009_10_PP.csv with 7217 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2009_10_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2023_24_PP.csv\n",
      "âœ… Loaded: MERGED2023_24_PP.csv with 6429 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2023_24_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: FieldOfStudyData1617_1718_PP.csv\n",
      "âœ… Loaded: FieldOfStudyData1617_1718_PP.csv with 264798 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: FieldOfStudyData1617_1718_PP\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2018_19_PP.csv\n",
      "âœ… Loaded: MERGED2018_19_PP.csv with 6807 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2018_19_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2017_18_PP.csv\n",
      "âœ… Loaded: MERGED2017_18_PP.csv with 7112 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2017_18_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2000_01_PP.csv\n",
      "âœ… Loaded: MERGED2000_01_PP.csv with 6654 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2000_01_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: FieldOfStudyData1718_1819_PP.csv\n",
      "âœ… Loaded: FieldOfStudyData1718_1819_PP.csv with 228757 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: FieldOfStudyData1718_1819_PP\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2001_02_PP.csv\n",
      "âœ… Loaded: MERGED2001_02_PP.csv with 6725 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2001_02_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2019_20_PP.csv\n",
      "âœ… Loaded: MERGED2019_20_PP.csv with 6694 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2019_20_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2016_17_PP.csv\n",
      "âœ… Loaded: MERGED2016_17_PP.csv with 7238 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2016_17_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED1997_98_PP.csv\n",
      "âœ… Loaded: MERGED1997_98_PP.csv with 6934 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED1997_98_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED1998_99_PP.csv\n",
      "âœ… Loaded: MERGED1998_99_PP.csv with 6702 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED1998_99_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2004_05_PP.csv\n",
      "âœ… Loaded: MERGED2004_05_PP.csv with 6747 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2004_05_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2021_22_PP.csv\n",
      "âœ… Loaded: MERGED2021_22_PP.csv with 6543 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2021_22_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2002_03_PP.csv\n",
      "âœ… Loaded: MERGED2002_03_PP.csv with 6652 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2002_03_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: FieldOfStudyData1920_2021_PP.csv\n",
      "âœ… Loaded: FieldOfStudyData1920_2021_PP.csv with 229188 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: FieldOfStudyData1920_2021_PP\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2015_16_PP.csv\n",
      "âœ… Loaded: MERGED2015_16_PP.csv with 7666 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2015_16_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2013_14_PP.csv\n",
      "âœ… Loaded: MERGED2013_14_PP.csv with 7869 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2013_14_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2007_08_PP.csv\n",
      "âœ… Loaded: MERGED2007_08_PP.csv with 6971 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2007_08_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2008_09_PP.csv\n",
      "âœ… Loaded: MERGED2008_09_PP.csv with 7055 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2008_09_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2022_23_PP.csv\n",
      "âœ… Loaded: MERGED2022_23_PP.csv with 6484 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2022_23_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED2010_11_PP.csv\n",
      "âœ… Loaded: MERGED2010_11_PP.csv with 7470 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED2010_11_PP.csv\n",
      "\n",
      "ğŸ“‚ Processing file: FieldOfStudyData1415_1516_PP.csv\n",
      "âœ… Loaded: FieldOfStudyData1415_1516_PP.csv with 214885 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: FieldOfStudyData1415_1516_PP\n",
      "\n",
      "ğŸ“‚ Processing file: CW2023_prelim.xlsx\n",
      "âœ… Loaded: CW2023_prelim.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2023_prelim.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2006.xlsx\n",
      "âœ… Loaded: CW2006.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2006.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2010.xlsx\n",
      "âœ… Loaded: CW2010.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2010.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2011.xlsx\n",
      "âœ… Loaded: CW2011.xlsx with 21 rows and 5 columns\n",
      "ğŸ“Š Summary written to sheet: CW2011.xlsx\n",
      "\n",
      "ğŸ“‚ Processing file: CW2007.xlsx\n",
      "âœ… Loaded: CW2007.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2007.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2000.xlsx\n",
      "âœ… Loaded: CW2000.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2000.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2016.xlsx\n",
      "âœ… Loaded: CW2016.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2016.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2020.xlsx\n",
      "âœ… Loaded: CW2020.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2020.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2021.xlsx\n",
      "âœ… Loaded: CW2021.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2021.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2022_preliminary.xlsx\n",
      "âœ… Loaded: CW2022_preliminary.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2022_preliminary.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2017.xlsx\n",
      "âœ… Loaded: CW2017.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2017.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2001.xlsx\n",
      "âœ… Loaded: CW2001.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2001.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2014.xlsx\n",
      "âœ… Loaded: CW2014.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2014.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2002.xlsx\n",
      "âœ… Loaded: CW2002.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2002.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2018.xlsx\n",
      "âœ… Loaded: CW2018.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2018.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2019.xlsx\n",
      "âœ… Loaded: CW2019.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2019.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2003.xlsx\n",
      "âœ… Loaded: CW2003.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2003.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2015.xlsx\n",
      "âœ… Loaded: CW2015.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2015.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2008.xlsx\n",
      "âœ… Loaded: CW2008.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2008.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2012.xlsx\n",
      "âœ… Loaded: CW2012.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2012.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2004.xlsx\n",
      "âœ… Loaded: CW2004.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2004.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2005.xlsx\n",
      "âœ… Loaded: CW2005.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2005.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2013.xlsx\n",
      "âœ… Loaded: CW2013.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2013.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: CW2009.xlsx\n",
      "âœ… Loaded: CW2009.xlsx with 21 rows and 3 columns\n",
      "âŒ Error processing CW2009.xlsx: No objects to concatenate\n",
      "\n",
      "ğŸ“‚ Processing file: FieldOfStudyData1415_1516_PP-checkpoint.csv\n",
      "âœ… Loaded: FieldOfStudyData1415_1516_PP-checkpoint.csv with 214885 rows and 174 columns\n",
      "ğŸ“Š Summary written to sheet: FieldOfStudyData1415_151_1\n",
      "\n",
      "ğŸ“‚ Processing file: MERGED1996_97_PP-checkpoint.csv\n",
      "âœ… Loaded: MERGED1996_97_PP-checkpoint.csv with 7007 rows and 3306 columns\n",
      "ğŸ“Š Summary written to sheet: MERGED1996_97_PP-checkpoint.\n",
      "\n",
      "âœ… All summaries saved to 'descriptive_summaries.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Load dictionary and discovered file paths\n",
    "data_dict = pd.read_excel(\"data_dictionary.xlsx\")\n",
    "\n",
    "with open(\"discovered_data_files.txt\", \"r\") as f:\n",
    "    all_paths = [line.strip() for line in f if line.strip().endswith((\".csv\", \".xlsx\", \".xls\"))]\n",
    "\n",
    "# Map short file names to full paths\n",
    "filename_to_path = {os.path.basename(p): p for p in all_paths}\n",
    "\n",
    "# Prepare Excel writer\n",
    "writer = pd.ExcelWriter(\"descriptive_summaries.xlsx\", engine=\"xlsxwriter\")\n",
    "used_sheet_names = set()\n",
    "\n",
    "for filename in data_dict[\"file\"].dropna().unique():\n",
    "    print(f\"\\nğŸ“‚ Processing file: {filename}\")\n",
    "    try:\n",
    "        full_path = filename_to_path.get(filename)\n",
    "        if not full_path:\n",
    "            raise FileNotFoundError(f\"Path for {filename} not found in discovered_data_files.txt\")\n",
    "\n",
    "        # Read file with low_memory=False to avoid dtype warnings\n",
    "        if full_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(full_path, low_memory=False)\n",
    "        elif full_path.endswith((\".xlsx\", \".xls\")):\n",
    "            df = pd.read_excel(full_path)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Skipping unsupported file type: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"âœ… Loaded: {filename} with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "        # ---- Numeric Summary ----\n",
    "        numeric_summary = df.describe(include=[int, float]).T\n",
    "        numeric_summary[\"missing_values\"] = df[numeric_summary.index].isnull().sum()\n",
    "        numeric_summary[\"data_type\"] = df[numeric_summary.index].dtypes.astype(str)\n",
    "\n",
    "        # ---- Categorical Summary ----\n",
    "        cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        cat_summary = pd.DataFrame(columns=[\"count\", \"unique\", \"top\", \"freq\", \"missing_values\", \"data_type\"])\n",
    "\n",
    "        for col in cat_cols:\n",
    "            try:\n",
    "                top_val = str(df[col].mode(dropna=True).iloc[0]) if not df[col].mode(dropna=True).empty else None\n",
    "                freq = df[col].value_counts(dropna=True).iloc[0] if not df[col].value_counts(dropna=True).empty else None\n",
    "            except Exception as e:\n",
    "                top_val = f\"Error: {e}\"\n",
    "                freq = None\n",
    "\n",
    "            cat_summary.loc[col] = {\n",
    "                \"count\": df[col].count(),\n",
    "                \"unique\": df[col].nunique(),\n",
    "                \"top\": top_val,\n",
    "                \"freq\": freq,\n",
    "                \"missing_values\": df[col].isnull().sum(),\n",
    "                \"data_type\": str(df[col].dtype)\n",
    "            }\n",
    "\n",
    "        # ---- Combine & Save ----\n",
    "        combined = pd.concat([numeric_summary, cat_summary])\n",
    "        combined.index.name = \"column\"\n",
    "\n",
    "        sheet_name = filename[:28]\n",
    "        suffix = 1\n",
    "        while sheet_name.lower() in used_sheet_names:\n",
    "            sheet_name = f\"{filename[:24]}_{suffix}\"\n",
    "            suffix += 1\n",
    "        used_sheet_names.add(sheet_name.lower())\n",
    "\n",
    "        combined.to_excel(writer, sheet_name=sheet_name)\n",
    "        print(f\"ğŸ“Š Summary written to sheet: {sheet_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {filename}: {e}\")\n",
    "        pd.DataFrame({\"error\": [str(e)]}).to_excel(writer, sheet_name=filename[:31])\n",
    "\n",
    "writer.close()\n",
    "print(\"\\nâœ… All summaries saved to 'descriptive_summaries.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a872765",
   "metadata": {},
   "source": [
    "### ğŸ“ OPTIONAL: Deep Dive into a Specific State or Dataset\n",
    "\n",
    "This section helps you analyze a particular dataset or a subset of data â€” such as institutions in a specific state.\n",
    "\n",
    "**To use:**\n",
    "\n",
    "1. Replace the `your_target_file.csv` with the full path to the dataset you want to analyze.\n",
    "2. (Optional) Apply a filter to look at a specific condition (e.g., `STATE == 'CA'`).\n",
    "\n",
    "> âœ… *Example: Looking only at data for California institutions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e215df1-4de6-4e7c-a137-5207c8374011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ” Load One Dataset ===\n",
    "# Replace this with the full path to your target file\n",
    "df_sample = pd.read_csv(\"your_target_file.csv\", low_memory=False)\n",
    "\n",
    "# Optional: Filter for a specific condition\n",
    "# Example: Filter for California institutions\n",
    "# df_sample = df_sample[df_sample[\"STATE\"] == \"CA\"]\n",
    "\n",
    "print(f\"âœ“ Loaded dataset with {df_sample.shape[0]} rows and {df_sample.shape[1]} columns\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc95c2-7322-4317-9788-022d77c3b2ad",
   "metadata": {},
   "source": [
    "#### ğŸ“‰ Visualize Missing Data\n",
    "\n",
    "This chart shows the top 30 columns with the highest fraction of missing values.\n",
    "\n",
    "This is useful for assessing the quality of your dataset and deciding which columns may need cleaning or removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85886fd-38b2-4834-9c63-a13cd9285075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data percentages for a sample dataset\n",
    "df_sample = pd.read_csv(\"your_target_file.csv\", low_memory=False)  # Replace with one of your full paths\n",
    "\n",
    "missing = df_sample.isnull().mean().sort_values(ascending=False)[:30]  # Top 30 only\n",
    "missing.plot(kind='barh', figsize=(10, 8), title=\"Top Missing Value Columns\")\n",
    "plt.xlabel(\"Fraction of Missing Values\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a5d62-eecf-4fbf-8614-94d2338c4c88",
   "metadata": {},
   "source": [
    "#### ğŸ“Š Explore Categorical Columns\n",
    "\n",
    "Quickly count the top values in any categorical column to get a feel for distributions.\n",
    "\n",
    "> âœ… *Example: Count institutions by state using `STATE` column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7049d5-44f8-4d43-be26-6d5e4a982421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ§® Top Values in a Categorical Column ===\n",
    "cat_col = \"STATE\"  # Replace with your column of interest\n",
    "df_sample[cat_col].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4a794-32af-4b12-923d-01b3a7a3e0f9",
   "metadata": {},
   "source": [
    "#### ğŸ—‚ï¸ Export Profile Summary for One Dataset\n",
    "\n",
    "Creates a basic summary of each column in the dataset, including:\n",
    "\n",
    "- Column name  \n",
    "- Data type  \n",
    "- % missing values  \n",
    "- Number of unique values\n",
    "\n",
    "ğŸ“ Saved as: `one_file_profile.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba957fb-9c48-48d1-b3a8-1d1aa18847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ—‚ï¸ Save Dataset Profile to Excel ===\n",
    "profile_df = pd.DataFrame({\n",
    "    \"column\": df_sample.columns,\n",
    "    \"dtype\": df_sample.dtypes.astype(str),\n",
    "    \"missing_pct\": df_sample.isnull().mean().round(3),\n",
    "    \"n_unique\": df_sample.nunique()\n",
    "})\n",
    "profile_df.to_excel(\"one_file_profile.xlsx\", index=False)\n",
    "print(\"âœ“ Profile saved as 'one_file_profile.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663b769-f071-4f65-8ca9-37609ac9619b",
   "metadata": {},
   "source": [
    "#### ğŸ”¬ Filter to Cleaned Numeric Subset\n",
    "\n",
    "Exports only numeric columns with less than 20% missing data.\n",
    "\n",
    "This is helpful when preparing a cleaner subset for modeling or visualization.\n",
    "\n",
    "ğŸ“ Saved as: `filtered_clean_numeric_subset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9774f5-8712-443e-b8d0-91a3c2e3fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ğŸ”¬ Export Cleaned Numeric Columns ===\n",
    "filtered = df_sample.loc[:, df_sample.select_dtypes(include=[int, float]).isnull().mean() < 0.2]\n",
    "filtered.to_csv(\"filtered_clean_numeric_subset.csv\", index=False)\n",
    "print(\"âœ“ Cleaned numeric subset saved as 'filtered_clean_numeric_subset.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
