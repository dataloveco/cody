


from pathlib import Path
import pandas as pd
import os

# Define the base directory path (change to whatever path your notebook is located)
base_path = Path(
    "/Users/jasminemotupalli/Library/CloudStorage/GoogleDrive-jasmine@dataloveco.com/"
    "My Drive/Personal/0 - U of Denver/4 - Term Summer 25/FIN-6305 Quant Methods III/Cody"
)

base_path


# Recursively list all files and directories in the base path
all_files = list(base_path.rglob("*"))
print(f"Total items found: {len(all_files)}")

# Show the first 20 items
for f in all_files[:20]:
    print(f)



# Filter to only data-like files (CSV, TXT, Excel)
data_files = [f for f in all_files if f.suffix in [".csv", ".txt", ".xlsx", ".xls"]]
print(f"Total explorable files found: {len(data_files)}")

# Show the data files
for f in data_files:
    print(f)


# Write the list of data files to a text file
output_path = base_path / "discovered_data_files.txt"

with open(output_path, "w") as f:
    f.write("Explorable Data Files Found:\n\n")
    for file in data_files:
        f.write(str(file) + "\n")

print(f"‚úì File list exported to: {output_path}")


# üìÑ Step 1: Load Discovered File Paths
file_path = "discovered_data_files.txt"  # Update if needed

with open(file_path, "r") as f:
    all_lines = f.readlines()

# Filter to get only valid data file paths
data_file_paths = [
    line.strip() for line in all_lines
    if line.strip().endswith(('.csv', '.xlsx', '.xls'))
]

# üß† Step 2: Loop Through Each File and Extract Metadata
data_dict = []

for path in data_file_paths:
    try:
        # Load first 5 rows depending on file type
        if path.endswith(".csv"):
            df = pd.read_csv(path, nrows=5)
        elif path.endswith((".xlsx", ".xls")):
            df = pd.read_excel(path, nrows=5)
        else:
            continue

        # Extract info for each column
        for col in df.columns:
            data_dict.append({
                "file": path.split("/")[-1],
                "column": col,
                "sample_value": df[col].iloc[0] if not df[col].empty else "N/A",
                "data_type": str(df[col].dtype)
            })

    except Exception as e:
        # If the file can't be read, log the error
        data_dict.append({
            "file": path.split("/")[-1],
            "column": "ERROR",
            "sample_value": str(e),
            "data_type": "N/A"
        })

# üìä Step 3: Convert to DataFrame
data_dict_df = pd.DataFrame(data_dict)

# üìÅ Step 4: Export to Excel
output_path = "data_dictionary.xlsx"
data_dict_df.to_excel(output_path, index=False)

# ‚úÖ Done
print(f"‚úì Data dictionary created: {output_path}")
data_dict_df.head()


# Load discovered file paths
file_path = "discovered_data_files.txt"

with open(file_path, "r") as f:
    all_lines = f.readlines()

# Get valid file paths
data_file_paths = [
    line.strip() for line in all_lines
    if line.strip().endswith((".csv", ".xlsx", ".xls"))
]

# Create a new Excel writer
output_excel = "multi_tab_variable_names.xlsx"
writer = pd.ExcelWriter(output_excel, engine="xlsxwriter")

# Track lowercase sheet names to avoid duplicates (Excel is case-insensitive)
used_sheet_names = set()

for path in data_file_paths:
    try:
        # Load first row to get column names
        if path.endswith(".csv"):
            df = pd.read_csv(path, nrows=1)
        elif path.endswith((".xlsx", ".xls")):
            df = pd.read_excel(path, nrows=1)
        else:
            continue

        # Prepare the dataframe of column names
        columns_df = pd.DataFrame({"column_name": df.columns})

        # Generate base sheet name
        base_name = os.path.basename(path).split(".")[0][:28]  # room for suffix
        sheet_name = base_name
        suffix = 1

        # Ensure uniqueness (case-insensitive)
        while sheet_name.lower() in used_sheet_names:
            sheet_name = f"{base_name}_{suffix}"[:31]
            suffix += 1

        used_sheet_names.add(sheet_name.lower())

        # Write to Excel
        columns_df.to_excel(writer, sheet_name=sheet_name, index=False)

    except Exception as e:
        error_df = pd.DataFrame({"column_name": [f"ERROR: {str(e)}"]})
        sheet_name = f"error_{os.path.basename(path)[:25]}"
        writer.sheets[sheet_name] = writer.book.add_worksheet(sheet_name)
        error_df.to_excel(writer, sheet_name=sheet_name, index=False)

# Save the workbook
writer.close()

print(f"‚úì New variable name Excel file created: {output_excel}")



# Load the data dictionary
data_dict = pd.read_excel("data_dictionary.xlsx")

# Get the list of unique files
unique_files = data_dict["file"].dropna().unique()

# Create an Excel writer to store results
summary_writer = pd.ExcelWriter("descriptive_summaries.xlsx", engine="xlsxwriter")

# Iterate over each file
for file_name in unique_files:
    file_path = file_name.strip()

    try:
        # Attempt to load the file (adjust path as needed for full access)
        if file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
        elif file_path.endswith((".xlsx", ".xls")):
            df = pd.read_excel(file_path)
        else:
            continue  # Skip unsupported formats

        # Create a summary dataframe
        summary_parts = []

        # ---- Descriptive Stats for Numeric Columns ----
        numeric_summary = df.describe(include=[int, float]).T
        numeric_summary["missing_values"] = df[numeric_summary.index].isnull().sum()
        numeric_summary["data_type"] = df[numeric_summary.index].dtypes.astype(str)
        summary_parts.append(numeric_summary)

        # ---- Summary for Categorical Columns ----
        categorical_cols = df.select_dtypes(include=["object", "category"]).columns
        cat_summary = pd.DataFrame(columns=["count", "unique", "top", "freq", "missing_values"])
        for col in categorical_cols:
            cat_summary.loc[col] = {
                "count": df[col].count(),
                "unique": df[col].nunique(),
                "top": df[col].mode().iloc[0] if not df[col].mode().empty else None,
                "freq": df[col].value_counts().iloc[0] if not df[col].value_counts().empty else None,
                "missing_values": df[col].isnull().sum()
            }
        cat_summary["data_type"] = df[categorical_cols].dtypes.astype(str)
        summary_parts.append(cat_summary)

        # Combine both into one sheet
        combined_summary = pd.concat(summary_parts)
        combined_summary.index.name = "column"

        # Excel sheet names must be <= 31 chars
        sheet_name = os.path.splitext(file_name)[0][:31]
        combined_summary.to_excel(summary_writer, sheet_name=sheet_name)

    except Exception as e:
        # Log the error if the file could not be read or processed
        pd.DataFrame({"error": [str(e)]}).to_excel(
            summary_writer, sheet_name=os.path.splitext(file_name)[0][:31]
        )

# Save the workbook
summary_writer.close()

print("‚úì Descriptive summaries saved to 'descriptive_summaries.xlsx'")

