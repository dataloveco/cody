





from pathlib import Path
import pandas as pd
import os
from collections import Counter
from collections import defaultdict
import matplotlib.pyplot as plt

# Define the base directory path (change to whatever path your notebook is located)
base_path = Path(
    "/Users/jasminemotupalli/Library/CloudStorage/GoogleDrive-jasmine@dataloveco.com/"
    "My Drive/Personal/0 - U of Denver/4 - Term Summer 25/FIN-6305 Quant Methods III/Cody"
)

base_path





# Recursively list all items (files + folders)
all_items = list(base_path.rglob("*"))
print(f"üîç Total items found: {len(all_items)}")

# Separate into files and directories
all_files = [p for p in all_items if p.is_file()]
all_dirs = [p for p in all_items if p.is_dir()]

print(f"üìÑ Total files: {len(all_files)}")
print(f"üìÅ Total folders: {len(all_dirs)}")

# Count file extensions
file_extensions = Counter(p.suffix.lower() for p in all_files if p.suffix)
print("\nüìä File extension breakdown (top 10):")
for ext, count in file_extensions.most_common(10):
    print(f"  {ext or '[no extension]':>8}: {count} files")

# Preview a few files
print("\nüóÇÔ∏è Sample files:")
for f in all_files[:10]:
    print(f"  - {f.relative_to(base_path)}")

# Preview a few folders
print("\nüìÇ Sample folders:")
for d in all_dirs[:5]:
    print(f"  - {d.relative_to(base_path)}")





# Define supported data extensions
data_extensions = [".csv", ".txt", ".xlsx", ".xls"]

# Filter files based on known data types
data_files = [f for f in all_files if f.suffix.lower() in data_extensions]
print(f"üìä Total explorable data files found: {len(data_files)}")

# Group data files by file type
ext_counts = defaultdict(int)
for f in data_files:
    ext_counts[f.suffix.lower()] += 1

# Show counts by type
print("\nüìÅ Breakdown by file type:")
for ext in data_extensions:
    print(f"  - {ext:6} : {ext_counts[ext]} files")

# Preview the first few data files
print("\nüìÇ Sample data files:")
for f in data_files[:10]:
    print(f"  - {f.relative_to(base_path)}")

# Warn about other files that were skipped
non_data_files = [f for f in all_files if f.is_file() and f.suffix.lower() not in data_extensions]
if non_data_files:
    print(f"\n‚ö†Ô∏è Note: {len(non_data_files)} files were skipped (not recognized as data files).")
    print("    Example skipped file:", non_data_files[0].name)





# Write the list of data files to a text file
output_path = base_path / "discovered_data_files.txt"

with open(output_path, "w") as f:
    f.write("Explorable Data Files Found:\n\n")
    for file in data_files:
        f.write(str(file) + "\n")

print(f"‚úì File list exported to: {output_path}")








# Load discovered file paths
file_path = "discovered_data_files.txt"

with open(file_path, "r") as f:
    all_lines = f.readlines()

# Get valid file paths
data_file_paths = [
    line.strip() for line in all_lines
    if line.strip().endswith((".csv", ".xlsx", ".xls"))
]

# Create a new Excel writer
output_excel = "multi_tab_variable_names.xlsx"
writer = pd.ExcelWriter(output_excel, engine="xlsxwriter")

# Track lowercase sheet names to avoid duplicates (Excel is case-insensitive)
used_sheet_names = set()

for path in data_file_paths:
    try:
        # Load first row to get column names
        if path.endswith(".csv"):
            df = pd.read_csv(path, nrows=1)
        elif path.endswith((".xlsx", ".xls")):
            df = pd.read_excel(path, nrows=1)
        else:
            continue

        # Prepare the dataframe of column names
        columns_df = pd.DataFrame({"column_name": df.columns})

        # Generate base sheet name
        base_name = os.path.basename(path).split(".")[0][:28]  # room for suffix
        sheet_name = base_name
        suffix = 1

        # Ensure uniqueness (case-insensitive)
        while sheet_name.lower() in used_sheet_names:
            sheet_name = f"{base_name}_{suffix}"[:31]
            suffix += 1

        used_sheet_names.add(sheet_name.lower())

        # Write to Excel
        columns_df.to_excel(writer, sheet_name=sheet_name, index=False)

    except Exception as e:
        error_df = pd.DataFrame({"column_name": [f"ERROR: {str(e)}"]})
        sheet_name = f"error_{os.path.basename(path)[:25]}"
        writer.sheets[sheet_name] = writer.book.add_worksheet(sheet_name)
        error_df.to_excel(writer, sheet_name=sheet_name, index=False)

# Save the workbook
writer.close()

print(f"‚úì New variable name Excel file created: {output_excel}")






# Load dictionary and discovered file paths
data_dict = pd.read_excel("data_dictionary.xlsx")

with open("discovered_data_files.txt", "r") as f:
    all_paths = [line.strip() for line in f if line.strip().endswith((".csv", ".xlsx", ".xls"))]

# Map short file names to full paths
filename_to_path = {os.path.basename(p): p for p in all_paths}

# Prepare Excel writer
writer = pd.ExcelWriter("descriptive_summaries.xlsx", engine="xlsxwriter")
used_sheet_names = set()

for filename in data_dict["file"].dropna().unique():
    print(f"\nüìÇ Processing file: {filename}")
    try:
        full_path = filename_to_path.get(filename)
        if not full_path:
            raise FileNotFoundError(f"Path for {filename} not found in discovered_data_files.txt")

        # Read file with low_memory=False to avoid dtype warnings
        if full_path.endswith(".csv"):
            df = pd.read_csv(full_path, low_memory=False)
        elif full_path.endswith((".xlsx", ".xls")):
            df = pd.read_excel(full_path)
        else:
            print(f"‚ö†Ô∏è Skipping unsupported file type: {filename}")
            continue

        print(f"‚úÖ Loaded: {filename} with {df.shape[0]} rows and {df.shape[1]} columns")

        # ---- Numeric Summary ----
        numeric_summary = df.describe(include=[int, float]).T
        numeric_summary["missing_values"] = df[numeric_summary.index].isnull().sum()
        numeric_summary["data_type"] = df[numeric_summary.index].dtypes.astype(str)

        # ---- Categorical Summary ----
        cat_cols = df.select_dtypes(include=["object", "category"]).columns
        cat_summary = pd.DataFrame(columns=["count", "unique", "top", "freq", "missing_values", "data_type"])

        for col in cat_cols:
            try:
                top_val = str(df[col].mode(dropna=True).iloc[0]) if not df[col].mode(dropna=True).empty else None
                freq = df[col].value_counts(dropna=True).iloc[0] if not df[col].value_counts(dropna=True).empty else None
            except Exception as e:
                top_val = f"Error: {e}"
                freq = None

            cat_summary.loc[col] = {
                "count": df[col].count(),
                "unique": df[col].nunique(),
                "top": top_val,
                "freq": freq,
                "missing_values": df[col].isnull().sum(),
                "data_type": str(df[col].dtype)
            }

        # ---- Combine & Save ----
        combined = pd.concat([numeric_summary, cat_summary])
        combined.index.name = "column"

        sheet_name = filename[:28]
        suffix = 1
        while sheet_name.lower() in used_sheet_names:
            sheet_name = f"{filename[:24]}_{suffix}"
            suffix += 1
        used_sheet_names.add(sheet_name.lower())

        combined.to_excel(writer, sheet_name=sheet_name)
        print(f"üìä Summary written to sheet: {sheet_name}")

    except Exception as e:
        print(f"‚ùå Error processing {filename}: {e}")
        pd.DataFrame({"error": [str(e)]}).to_excel(writer, sheet_name=filename[:31])

writer.close()
print("\n‚úÖ All summaries saved to 'descriptive_summaries.xlsx'")





# === üîç Load One Dataset ===
# Replace this with the full path to your target file
df_sample = pd.read_csv("your_target_file.csv", low_memory=False)

# Optional: Filter for a specific condition
# Example: Filter for California institutions
# df_sample = df_sample[df_sample["STATE"] == "CA"]

print(f"‚úì Loaded dataset with {df_sample.shape[0]} rows and {df_sample.shape[1]} columns")
df_sample.head()





# Visualize missing data percentages for a sample dataset
df_sample = pd.read_csv("your_target_file.csv", low_memory=False)  # Replace with one of your full paths

missing = df_sample.isnull().mean().sort_values(ascending=False)[:30]  # Top 30 only
missing.plot(kind='barh', figsize=(10, 8), title="Top Missing Value Columns")
plt.xlabel("Fraction of Missing Values")
plt.tight_layout()
plt.show()





# === üßÆ Top Values in a Categorical Column ===
cat_col = "STATE"  # Replace with your column of interest
df_sample[cat_col].value_counts(dropna=False).head(10)





# === üóÇÔ∏è Save Dataset Profile to Excel ===
profile_df = pd.DataFrame({
    "column": df_sample.columns,
    "dtype": df_sample.dtypes.astype(str),
    "missing_pct": df_sample.isnull().mean().round(3),
    "n_unique": df_sample.nunique()
})
profile_df.to_excel("one_file_profile.xlsx", index=False)
print("‚úì Profile saved as 'one_file_profile.xlsx'")





# === üî¨ Export Cleaned Numeric Columns ===
filtered = df_sample.loc[:, df_sample.select_dtypes(include=[int, float]).isnull().mean() < 0.2]
filtered.to_csv("filtered_clean_numeric_subset.csv", index=False)
print("‚úì Cleaned numeric subset saved as 'filtered_clean_numeric_subset.csv'")
